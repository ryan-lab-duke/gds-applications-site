
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Machine learning fundamentals &#8212; Geospatial Data Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/unit2/lecture-5';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Wine activity" href="../../activities/wines.html" />
    <link rel="prev" title="Assignment 4" href="../../labs/assignment4.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/nsoe-logo.png" class="logo__image only-light" alt="Geospatial Data Science - Home"/>
    <img src="../../_static/nsoe-logo.png" class="logo__image only-dark pst-js-only" alt="Geospatial Data Science - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to Geospatial Data Science!
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course information</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../course-info/general-info.html">General information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/schedule.html">Schedule (for Fall 2025)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/python.html">Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/github.html">GitHub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/format.html">Formatting answers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/final-project.html">Final projects</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit1/lecture-1.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../activities/rivers.html">Rivers of the World Activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../labs/assignment1.html">Assignment 1</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit1/lecture-2.html">Vector data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../activities/flooding.html">Residential Flooding Activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../labs/assignment2.html">Assignment 2</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit1/lecture-3.html">Network data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../activities/census.html">Census Activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../labs/assignment3.html">Assignment 3</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit1/lecture-4a.html">Gridded data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit1/lecture-4b.html">Climate data analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../labs/assignment4.html">Assignment 4</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 5</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Machine learning fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../activities/wines.html">Wine activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../labs/assignment5.html">Assignment 5</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../final-project/baby.html">Baby</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 7</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-6a.html">Machine learning applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-6b.html">Neural networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../activities/penguins.html">Penguin activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../labs/assignment6.html">Assignment 6</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 8</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../final-project/fall-break.html">Fall break</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../final-project/project-ideas.html">Assignment 7</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 9</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-7.html">Specialized neural networks</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../activities/eurosat.html">EuroSAT activity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../labs/assignment7.html">Assignment 8</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 10</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit3/lecture-8.html">Data access</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../final-project/readme.html">Make a README</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 11</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit3/lecture-9.html">Code management and collaboration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../final-project/initialize.html">Initialize GitHub repository</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 12</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit3/lecture-10.html">Automating raster analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../activities/contribute.html">Contributing to a GitHub repository</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 13</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../unit4/lecture-11a.html">Visualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../unit4/lecture-11b.html">Maps with data</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/ryan-lab-duke/gds-applications-site" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/ryan-lab-duke/gds-applications-site/issues/new?title=Issue%20on%20page%20%2Flectures/unit2/lecture-5.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/unit2/lecture-5.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine learning fundamentals</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">What is machine learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-the-palmer-penguins">Example - the Palmer Penguins</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-vocabularly">Some vocabularly</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning">Types of machine learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">Supervised learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning">Unsupervised learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">Reinforcement learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-machine-learning-tasks">Common machine learning tasks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification">Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression">Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-our-dataset">Exploring our dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">Feature scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-subsets">Training and testing subsets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-our-first-model">Fitting our first model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">Cross-validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree-classifier">Decision Tree Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-or-error-functions">Loss (or error) functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning">Hyperparameter tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#another-example-california-house-prices">Another example - California House Prices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#noise">Noise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-curves">Learning curves</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensembles">Ensembles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging">Bagging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting">Boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-boosting">Gradient boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="machine-learning-fundamentals">
<h1>Machine learning fundamentals<a class="headerlink" href="#machine-learning-fundamentals" title="Link to this heading">#</a></h1>
<p>This week we will be learning about the basic concepts of machine learning. The goal here is to understand several important concepts (as well as the terminology) that appear often in machine learning research.</p>
<section id="what-is-machine-learning">
<h2>What is machine learning?<a class="headerlink" href="#what-is-machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The goal of machine learning is to build <strong>predictive models</strong></p></li>
<li><p>These models <strong>learn</strong> from data to identify patterns and make predictions <strong>without being explicitly programmed</strong></p></li>
<li><p>Contrasts with process-based models that rely on established scientific principles and mathematical equations to describe a system’s behavior</p></li>
<li><p>Machine learning is <strong>part</strong> of artificial intelligence, but not the only part</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/ml_schematic.jpg"><img alt="../../_images/ml_schematic.jpg" class="align-center" src="../../_images/ml_schematic.jpg" style="width: 600px;" /></a>
</section>
<section id="example-the-palmer-penguins">
<h2>Example - the <a class="reference external" href="https://allisonhorst.github.io/palmerpenguins/index.html">Palmer Penguins</a><a class="headerlink" href="#example-the-palmer-penguins" title="Link to this heading">#</a></h2>
<a class="reference internal image-reference" href="../../_images/penguins1.png"><img alt="../../_images/penguins1.png" class="align-center" src="../../_images/penguins1.png" style="width: 600px;" /></a>
<p>Artwork by &#64;allison_horst</p>
<p>We will use the Penguin dataset to demonstrate some key concepts. This dataset contains attributes for 342 penguins collected from three islands in the Palmer Archipelago, Antarctica. More information about the Palmer Penguin dataset can be found <a class="reference external" href="https://allisonhorst.github.io/palmerpenguins/index.html">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import packages</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpl</span>

<span class="c1"># Read data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/penguins.csv&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(342, 5)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Culmen Length (mm)</th>
      <th>Culmen Depth (mm)</th>
      <th>Flipper Length (mm)</th>
      <th>Body Mass (g)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>39.1</td>
      <td>18.7</td>
      <td>181</td>
      <td>3750</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Adelie</td>
      <td>39.5</td>
      <td>17.4</td>
      <td>186</td>
      <td>3800</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Adelie</td>
      <td>40.3</td>
      <td>18.0</td>
      <td>195</td>
      <td>3250</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Adelie</td>
      <td>36.7</td>
      <td>19.3</td>
      <td>193</td>
      <td>3450</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Adelie</td>
      <td>39.3</td>
      <td>20.6</td>
      <td>190</td>
      <td>3650</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<a class="reference internal image-reference" href="../../_images/culmen_depth.png"><img alt="../../_images/culmen_depth.png" class="align-center" src="../../_images/culmen_depth.png" style="width: 600px;" /></a>
<p>Artwork by &#64;allison_horst</p>
<p>There are three different species of penguins in this dataset. Our goal is to develop a model that can predict the species of penguin from just the attributes (i.e. <strong>multi-class classification</strong>). Then, when we receive more <strong>unlabelled data</strong> (i.e. attributes but no species), we can make a prediction about the species of penguin.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Adelie&#39;, &#39;Gentoo&#39;, &#39;Chinstrap&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<p>One way to do this would be develop some decision rules.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Species&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Culmen Length (mm)</th>
      <th>Culmen Depth (mm)</th>
      <th>Flipper Length (mm)</th>
      <th>Body Mass (g)</th>
    </tr>
    <tr>
      <th>Species</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Adelie</th>
      <td>38.791391</td>
      <td>18.346358</td>
      <td>189.953642</td>
      <td>3700.662252</td>
    </tr>
    <tr>
      <th>Chinstrap</th>
      <td>48.833824</td>
      <td>18.420588</td>
      <td>195.823529</td>
      <td>3733.088235</td>
    </tr>
    <tr>
      <th>Gentoo</th>
      <td>47.504878</td>
      <td>14.982114</td>
      <td>217.186992</td>
      <td>5076.016260</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>It looks like Adelie penguins have a smaller bill length and that Gentoo penguins have longer flippers and are heavier.</p>
<ul class="simple">
<li><p>We could use our own judgement to define a threshold for separating the species based on their attributes. But this would be challenging to do when our datasets have more than a couple of attributes.</p></li>
<li><p>An alternative approach is to develop a model that <strong>learns the decision rules directly from the data</strong> (i.e. machine learning).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot histogram of Flipper Length for each species</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">species</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">():</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">species</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">species</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/3b583028fc0890b8abb74d176c88d9f0a4c23c739195e6b0fb0d0c410f6b59ae.png" src="../../_images/3b583028fc0890b8abb74d176c88d9f0a4c23c739195e6b0fb0d0c410f6b59ae.png" />
</div>
</div>
</section>
<section id="some-vocabularly">
<h2>Some vocabularly<a class="headerlink" href="#some-vocabularly" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Most machine learning (in Earth Sciences) starts with a <strong>labelled dataset</strong></p></li>
<li><p>A <strong>label</strong> (or target variable) is the thing we’re attempting to predict (e.g. penguin species)</p></li>
<li><p>A <strong>feature</strong> (or predictor variable) is an individual measurable property of the dataset (e.g. flipper length)</p></li>
<li><p>A <strong>sample</strong> are different observations or rows in the table</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Species</th>
      <th>Culmen Length (mm)</th>
      <th>Culmen Depth (mm)</th>
      <th>Flipper Length (mm)</th>
      <th>Body Mass (g)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>39.1</td>
      <td>18.7</td>
      <td>181</td>
      <td>3750</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Adelie</td>
      <td>39.5</td>
      <td>17.4</td>
      <td>186</td>
      <td>3800</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Adelie</td>
      <td>40.3</td>
      <td>18.0</td>
      <td>195</td>
      <td>3250</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Adelie</td>
      <td>36.7</td>
      <td>19.3</td>
      <td>193</td>
      <td>3450</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Adelie</td>
      <td>39.3</td>
      <td>20.6</td>
      <td>190</td>
      <td>3650</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="types-of-machine-learning">
<h2>Types of machine learning<a class="headerlink" href="#types-of-machine-learning" title="Link to this heading">#</a></h2>
<section id="supervised-learning">
<h3>Supervised learning<a class="headerlink" href="#supervised-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The goal is to <strong>train</strong> an algorithm using features in <strong>labelled</strong> dataset to predict the label of new data that is <strong>unlabelled</strong></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>That this <strong>label</strong> could be <strong>categorical</strong> (i.e. classification) or <strong>continuous</strong> (i.e. regression)</p>
</div>
<ul class="simple">
<li><p>However, it’s tough to collect high-quality labelled data (time-consuming, expensive), sometimes we don’t know what the labels should be.</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/captcha.jpeg"><img alt="../../_images/captcha.jpeg" class="align-center" src="../../_images/captcha.jpeg" style="width: 300px;" /></a>
</section>
<section id="unsupervised-learning">
<h3>Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The goal is to extract some structure from our <strong>unlabelled dataset</strong> that can be used to generalize.</p></li>
<li><p>For example, how many different species are contained in our penguin dataset?</p></li>
<li><p>Not commonly used in geospatial data science but has a wide variety of applications (e.g. next-token prediction)</p></li>
</ul>
</section>
<section id="reinforcement-learning">
<h3>Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>An agent learns to make decisions by interacting with an environment</p></li>
<li><p>For example, chess-playing like AlphaZero, self-driving cars</p></li>
</ul>
</section>
</section>
<section id="common-machine-learning-tasks">
<h2>Common machine learning tasks<a class="headerlink" href="#common-machine-learning-tasks" title="Link to this heading">#</a></h2>
<section id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“Split things into <strong>groups</strong> based on their <strong>features</strong>”</p></li>
<li><p>Examples include:</p>
<ul>
<li><p>Land cover</p></li>
<li><p>Flood risk zones</p></li>
<li><p>Sentiment analysis</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/classification.jpg"><img alt="../../_images/classification.jpg" class="align-center" src="../../_images/classification.jpg" style="width: 300px;" /></a>
</section>
<section id="regression">
<h3>Regression<a class="headerlink" href="#regression" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“Draw a <strong>line</strong> through these dots”</p></li>
<li><p>Used for predicting continuous variables:</p>
<ul>
<li><p>River discharge</p></li>
<li><p>House prices</p></li>
<li><p>Weather forecasting</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/regression.jpeg"><img alt="../../_images/regression.jpeg" class="align-center" src="../../_images/regression.jpeg" style="width: 300px;" /></a>
</section>
<section id="clustering">
<h3>Clustering<a class="headerlink" href="#clustering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>“Divide data into groups but machine chooses the best way”</p></li>
<li><p>Common usages include:</p>
<ul>
<li><p>Image compression</p></li>
<li><p>Labeling training data (i.e. for supervised learning)</p></li>
<li><p>Detecting abnormal behavior</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/clustering.jpeg"><img alt="../../_images/clustering.jpeg" class="align-center" src="../../_images/clustering.jpeg" style="width: 300px;" /></a>
</section>
</section>
<section id="exploring-our-dataset">
<h2>Exploring our dataset<a class="headerlink" href="#exploring-our-dataset" title="Link to this heading">#</a></h2>
<p>The first step when developing any machine learning model is to gain some intuition about our dataset. <code class="docutils literal notranslate"><span class="pre">Pandas</span></code> contains some ueful functions for doing this.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 342 entries, 0 to 341
Data columns (total 5 columns):
 #   Column               Non-Null Count  Dtype  
---  ------               --------------  -----  
 0   Species              342 non-null    object 
 1   Culmen Length (mm)   342 non-null    float64
 2   Culmen Depth (mm)    342 non-null    float64
 3   Flipper Length (mm)  342 non-null    int64  
 4   Body Mass (g)        342 non-null    int64  
dtypes: float64(2), int64(2), object(1)
memory usage: 13.5+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Culmen Length (mm)</th>
      <th>Culmen Depth (mm)</th>
      <th>Flipper Length (mm)</th>
      <th>Body Mass (g)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>342.000000</td>
      <td>342.000000</td>
      <td>342.000000</td>
      <td>342.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>43.921930</td>
      <td>17.151170</td>
      <td>200.915205</td>
      <td>4201.754386</td>
    </tr>
    <tr>
      <th>std</th>
      <td>5.459584</td>
      <td>1.974793</td>
      <td>14.061714</td>
      <td>801.954536</td>
    </tr>
    <tr>
      <th>min</th>
      <td>32.100000</td>
      <td>13.100000</td>
      <td>172.000000</td>
      <td>2700.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>39.225000</td>
      <td>15.600000</td>
      <td>190.000000</td>
      <td>3550.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>44.450000</td>
      <td>17.300000</td>
      <td>197.000000</td>
      <td>4050.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>48.500000</td>
      <td>18.700000</td>
      <td>213.000000</td>
      <td>4750.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>59.600000</td>
      <td>21.500000</td>
      <td>231.000000</td>
      <td>6300.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can also investigate the <strong>separability</strong> of penguin species using scatterplots</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Scatter plot categorized by species</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">species_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="k">for</span> <span class="n">species</span> <span class="ow">in</span> <span class="n">species_list</span><span class="p">:</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">species</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span> 
        <span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">],</span> 
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> 
        <span class="n">label</span><span class="o">=</span><span class="n">species</span>
    <span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Body Mass (g)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x1323ffef0&gt;
</pre></div>
</div>
<img alt="../../_images/3850053b59d643122678154c13fddeb51b6f8b422b31cee406f9de9c7cab1245.png" src="../../_images/3850053b59d643122678154c13fddeb51b6f8b422b31cee406f9de9c7cab1245.png" />
</div>
</div>
<p>Finally, we may also want to check the number of observations in each category.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Species&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Species
Adelie       151
Gentoo       123
Chinstrap     68
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>We find that there are less Chinstrap penguins than other types in this dataset. This is example <strong>class imbalance</strong> which we should be aware of when developing our model. For example, our model could have a high overall accuracy even if it is not be very good at identifying Chinstraps. This may mask the true performance of our model, especially for specific questions about Chinstrap penguins.</p>
</section>
<section id="feature-scaling">
<h2>Feature scaling<a class="headerlink" href="#feature-scaling" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Machine Learning algorithms sometimes don’t perform well when the input numerical attributes have very <strong>different scales</strong>.</p></li>
<li><p>Therefore we often <strong>scale</strong> (or normalize) our features before training the model (e.g. min-max scaling or standardization).</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Models that rely on the <strong>distance</strong> between a pair of samples (e.g. k-nearest neighbors), should be trained on normalized features to make each feature contribute approximately equally to the distance computations. In general, it is good practice to normalize everything.</p>
</div>
<ul class="simple">
<li><p><strong>Min-max method</strong> scales values so that they end up ranging from 0 to 1</p></li>
<li><p><strong>Standardization</strong> scales values so that the they have mean of 0 and unit variance.</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/scaling.png"><img alt="../../_images/scaling.png" class="align-center" src="../../_images/scaling.png" style="width: 800px;" /></a>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import package</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Define feature list</span>
<span class="n">feature_list</span> <span class="o">=</span>  <span class="p">[</span><span class="s1">&#39;Culmen Length (mm)&#39;</span><span class="p">,</span> <span class="s1">&#39;Culmen Depth (mm)&#39;</span><span class="p">,</span> <span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">,</span> <span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">]</span>

<span class="c1"># Define features and labels </span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_list</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standarize data</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>  
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">df_scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">feature_list</span><span class="p">)</span>
<span class="n">df_scaled</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Culmen Length (mm)</th>
      <th>Culmen Depth (mm)</th>
      <th>Flipper Length (mm)</th>
      <th>Body Mass (g)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.884499</td>
      <td>0.785449</td>
      <td>-1.418347</td>
      <td>-0.564142</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.811126</td>
      <td>0.126188</td>
      <td>-1.062250</td>
      <td>-0.501703</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.664380</td>
      <td>0.430462</td>
      <td>-0.421277</td>
      <td>-1.188532</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-1.324737</td>
      <td>1.089724</td>
      <td>-0.563715</td>
      <td>-0.938776</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.847812</td>
      <td>1.748985</td>
      <td>-0.777373</td>
      <td>-0.689020</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_scaled</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Culmen Length (mm)</th>
      <th>Culmen Depth (mm)</th>
      <th>Flipper Length (mm)</th>
      <th>Body Mass (g)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>3.420000e+02</td>
      <td>3.420000e+02</td>
      <td>3.420000e+02</td>
      <td>3.420000e+02</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>1.662088e-16</td>
      <td>4.155221e-16</td>
      <td>-8.310441e-16</td>
      <td>8.310441e-17</td>
    </tr>
    <tr>
      <th>std</th>
      <td>1.001465e+00</td>
      <td>1.001465e+00</td>
      <td>1.001465e+00</td>
      <td>1.001465e+00</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-2.168526e+00</td>
      <td>-2.054446e+00</td>
      <td>-2.059320e+00</td>
      <td>-1.875362e+00</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-8.615697e-01</td>
      <td>-7.866355e-01</td>
      <td>-7.773731e-01</td>
      <td>-8.138982e-01</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>9.686524e-02</td>
      <td>7.547549e-02</td>
      <td>-2.788381e-01</td>
      <td>-1.895079e-01</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>8.397670e-01</td>
      <td>7.854492e-01</td>
      <td>8.606705e-01</td>
      <td>6.846384e-01</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2.875868e+00</td>
      <td>2.205397e+00</td>
      <td>2.142618e+00</td>
      <td>2.620248e+00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="training-and-testing-subsets">
<h2>Training and testing subsets<a class="headerlink" href="#training-and-testing-subsets" title="Link to this heading">#</a></h2>
<p>If we want to properly evaluate our machine learning model, we should not use <strong>all</strong> the data for training. If we did that, the model may be able to just <strong>memorize</strong> the dataset and would struggle predicting new, unseen data.</p>
<p>Instead, we should train the model on a <strong>subset of the data</strong>, retaining another subset (that the model has not “seen”) to <strong>evaluate</strong> the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split data (80% train, 20% test)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We split our data <strong>randomly</strong> so that 80% is used for training (<code class="docutils literal notranslate"><span class="pre">X_train</span></code> and <code class="docutils literal notranslate"><span class="pre">y_train</span></code>) and 20% is used for testing (<code class="docutils literal notranslate"><span class="pre">X_test</span></code> and <code class="docutils literal notranslate"><span class="pre">y_test</span></code>). Our test dataset therfore contains 69 individuals (and associated features), 11 of which are Chinstrap penguins.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_test</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Species
Adelie       35
Gentoo       23
Chinstrap    11
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
<section id="fitting-our-first-model">
<h2>Fitting our first model<a class="headerlink" href="#fitting-our-first-model" title="Link to this heading">#</a></h2>
<p>Now we are ready to build our first model. We will first experiment with a very simple supervised algorithm that classifies our data using <strong>K-nearest neighbors</strong>. This algorithm computes the distance between a new data point and all training points. It then finds the closest <strong>k-neighbors</strong> (usually Euclidean but could be Manhattan) and uses a majority vote to assign a label to the new data point.</p>
<a class="reference internal image-reference" href="../../_images/k-nearest.png"><img alt="../../_images/k-nearest.png" class="align-center" src="../../_images/k-nearest.png" style="width: 500px;" /></a>
<p>Let’s define the K-nearest neighbors classifer in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># Define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Once we have defined the model there are three main steps to using it: <code class="docutils literal notranslate"><span class="pre">fit</span></code>, <code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">score</span></code>. The <code class="docutils literal notranslate"><span class="pre">fit</span></code> method is called to train the model from the input (features) and target data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit model to training data</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>KNeighborsClassifier</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">?<span>Documentation for KNeighborsClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div> </div></div></div></div></div></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> an object that has a <code class="docutils literal notranslate"><span class="pre">fit</span></code> method is called an <strong>estimator</strong>. We can use the estimator to make predictions on our data.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict test labels</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We then use the <code class="docutils literal notranslate"><span class="pre">score</span></code> method to evaluate the performance of our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Score</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The test accuracy using a </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The test accuracy using a KNeighborsClassifier is 0.986
</pre></div>
</div>
</div>
</div>
<p>To compute the score, the predictor first computes the predictions (using the predict method) and then uses a scoring function to compare the true value vs. the predictions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default scoring function for classifiers is <strong>mean accuracy</strong> defined as the number of correct predictions divided by the total number of samples.</p>
</div>
<p>If we compare with the accuracy obtained by wrongly evaluating the model on the <strong>training data</strong>, we find a higher accuracy compared to the score obtained on the held-out test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The test accuracy using a </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The test accuracy using a KNeighborsClassifier is 0.996
</pre></div>
</div>
</div>
</div>
</section>
<section id="confusion-matrix">
<h2>Confusion matrix<a class="headerlink" href="#confusion-matrix" title="Link to this heading">#</a></h2>
<p>Mean accuracy is a rather crude metric for evaluating the performance of our model. Each observation in the test dataset is treated equally, So, as we mentioned earlier, the model could be 100% accurate at identifying common penguin species (e.g. Adelie and Gentoo) but be bad at identifying rarer penguin species (e.g. Chinstrap) and still have a high overall accuracy.</p>
<p>The standard way of evaluating a multi-class prediction model is to use a confusion matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>

<span class="c1"># Generate confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8733911ce286bbb5504b66c6e45d5f966224497920396f38b9b39d0021057396.png" src="../../_images/8733911ce286bbb5504b66c6e45d5f966224497920396f38b9b39d0021057396.png" />
</div>
</div>
<p>Luckily we didn’t need to be worried about class imbalances. It looks like all the Chinstrap penguins in the test dataset were labelled accurately. However, our model mislabelled one Adelie penguin as a Chinstrap.</p>
</section>
<section id="cross-validation">
<h2>Cross-validation<a class="headerlink" href="#cross-validation" title="Link to this heading">#</a></h2>
<p>Since our dataset has 342 individuals (i.e. not that many), we might as well use them all to fit our model. But, if we do this, we won’t be able to effectively evaluate our model. To overcome this, we can use a technique called <strong>cross-validation</strong> which involves systematically repeating the train-test split such that the training and testing datasets are different for each evaluation.</p>
<a class="reference internal image-reference" href="../../_images/kfold.png"><img alt="../../_images/kfold.png" class="align-center" src="../../_images/kfold.png" style="width: 800px;" /></a>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="c1"># Define cross-validation</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Perform cross-validation</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The mean test accuracy using cross-validation is </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean test accuracy using cross-validation is 0.988
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When we set <code class="docutils literal notranslate"><span class="pre">n_splits</span></code> to <strong>5</strong>, the dataset is split into 5 roughly equal parts (“folds”). In each round, 4 folds are used for training and 1 fold is used for testing (i.e. 80/20 split). If we set <code class="docutils literal notranslate"><span class="pre">n_splits</span></code> to 10, then the dataset would be split into 10 folds, meaning that 9 would be used for training and 1 for testing (i.e. 90/10 split). The split is therefore dependent on the number of folds. <code class="docutils literal notranslate"><span class="pre">KFold</span></code> may not be appropriate for small datasets (since <code class="docutils literal notranslate"><span class="pre">n_splits</span></code> cannot be larger than number of samples).</p>
</div>
<p>An alternative approach is called the <code class="docutils literal notranslate"><span class="pre">ShuffleSplit</span></code> which, as the name suggests, shuffles the dataset randomly and then splits it into a training set and a test set according to the sizes that we choose. This approach allows us to define specific train-test splits.</p>
<a class="reference internal image-reference" href="../../_images/shufflesplit.png"><img alt="../../_images/shufflesplit.png" class="align-center" src="../../_images/shufflesplit.png" style="width: 800px;" /></a>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">ShuffleSplit</span>

<span class="c1"># Define cross-validation</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Perform cross-validation</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The mean test accuracy using cross-validation is </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The mean test accuracy using cross-validation is 0.991
</pre></div>
</div>
</div>
</div>
</section>
<section id="decision-tree-classifier">
<h2>Decision Tree Classifier<a class="headerlink" href="#decision-tree-classifier" title="Link to this heading">#</a></h2>
<p>K-nearest Neighbors is an odd type of model because it doesn’t actually learn during training. It simply stores the training data and makes predictions based on distances to a specific number of neigbors at inference time. But now that we have formatted our data for K-nearest neighbors, we can easily test other, more conventional machine learning models available in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="c1"># Define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Fit model</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Score</span>
<span class="n">test_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The test accuracy using a </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The test accuracy using a DecisionTreeClassifier is 0.953
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict test labels</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Generate confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7dbab7f564353be6ded9985945cacc0fe21328e0ef92b1e89d7382f50d786df5.png" src="../../_images/7dbab7f564353be6ded9985945cacc0fe21328e0ef92b1e89d7382f50d786df5.png" />
</div>
</div>
<p>The nice thing about Decision Trees is that we can visualize the fitted model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_tree</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
          <span class="n">feature_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Culmen Length&#39;</span><span class="p">,</span> <span class="s1">&#39;Culmen Depth&#39;</span><span class="p">,</span> <span class="s1">&#39;Flipper Length&#39;</span><span class="p">,</span> <span class="s1">&#39;Body Mass&#39;</span><span class="p">],</span> 
          <span class="n">class_names</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">(),</span>
          <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">impurity</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>       
          <span class="n">proportion</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>        
          <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/cf41088551cf77274cfb3d9f0f9ad4e97241eb65dd725cceeabacd5ec6c746cb.png" src="../../_images/cf41088551cf77274cfb3d9f0f9ad4e97241eb65dd725cceeabacd5ec6c746cb.png" />
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The attribute (e.g. <code class="docutils literal notranslate"><span class="pre">Flipper</span> <span class="pre">Length</span></code>) can be positive or negative because we standardized the values.</p>
</div>
<p>Decision trees partition the feature space by considering a single feature at a time. We start with 273 samples (80% of dataset), if <code class="docutils literal notranslate"><span class="pre">Flipper</span> <span class="pre">Length</span></code> is smaller than <code class="docutils literal notranslate"><span class="pre">0.47</span></code> then we are left with 168 samples, most of which are now <code class="docutils literal notranslate"><span class="pre">Adelie</span></code>. Then, if <code class="docutils literal notranslate"><span class="pre">Culmen</span> <span class="pre">Length</span></code> is less than <code class="docutils literal notranslate"><span class="pre">-0.14</span></code>, we are left with <strong>116</strong> samples which we classify as <code class="docutils literal notranslate"><span class="pre">Adelie</span></code>. Unfortunately, it looks like <strong>4</strong> of these penguins are actually <code class="docutils literal notranslate"><span class="pre">Gentoos</span></code>.</p>
<p>We might be asking a couple more questions by now: 1) How does the model choose these thresholds and 2) what is <code class="docutils literal notranslate"><span class="pre">gini</span></code>?</p>
</section>
<section id="loss-or-error-functions">
<h2>Loss (or error) functions<a class="headerlink" href="#loss-or-error-functions" title="Link to this heading">#</a></h2>
<p>During the learning process, our Decision Tree is trying to find a set of thresholds that make the fewest mistakes when classifying our training data. To find the best thresholds, the algorithm evaluates different thresholds for each feature and selects the one that provides the least error relative to labels in the training dataset. The metric that we use to evaluate whether one set of thresholds are better than another is called a <strong>loss function</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since an exhaustive (i.e. brute-force) search over all real numbers is impossible, the algorithm only tests thresholds between consecutive unique values contained in the dataset.</p>
</div>
<p>Decision Trees commonly use the <strong>Gini loss function</strong> (or impurity). This index measures how mixed the classes are in a given group of samples (i.e. <strong>entropy</strong>). If a group of samples all belong to the same class, the Gini impurity is 0 (perfectly “pure”). If the group has a mix classes, the impurity is higher. At each split, the Decision Tree algorithm tries to reduce entropy or the minimize Gini impurity.</p>
<p>For regression, a common loss function is <strong>absolute loss</strong> (i.e. MAE) where loss grows <strong>linearly</strong> with absolute mispredicted amount. This loss function is approproate for noisy data, when some mispredictions are unavoidable and shouldn’t dominate the loss.</p>
<p>Another common loss function in regression is <strong>squared loss</strong> (i.e. MSE) where loss grows <strong>quadratically</strong> with the absolute mispredicted amount. If a prediction is very close to being correct, the square will be small and little attention will be given to that example to obtain zero error. On the other hand, the penalty will be large for predictions that are really far off.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate a range of prediction errors for regression losses</span>
<span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">errors</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Generate probabilities for classification impurity measures</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>  <span class="c1"># avoid log(0)</span>
<span class="n">gini</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">p</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span>
<span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">))</span>

<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gini</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gini Impurity&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">entropy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Entropy&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class probability&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Impurity / loss&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mean Absolute Error (MAE)&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mean Squared Error (MSE)&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Prediction error&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/b8ed39dd9d350c0dd37618f48ee24eaa3c116d981c4465ca5fdfcbf0a3c884f2.png" src="../../_images/b8ed39dd9d350c0dd37618f48ee24eaa3c116d981c4465ca5fdfcbf0a3c884f2.png" />
</div>
</div>
</section>
<section id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Link to this heading">#</a></h2>
<p>Both models that we have used so far can be <strong>customized</strong> using parameters. We can apply the <code class="docutils literal notranslate"><span class="pre">get_params()</span></code> method to our estimators to see what we can adjust.</p>
<p>For our <code class="docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code>, the most important parameter is <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> which is the number of neighboring observations that the algorithm looks at when making a prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span>

<span class="c1"># Print all hyperparameters</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;algorithm&#39;: &#39;auto&#39;,
 &#39;leaf_size&#39;: 30,
 &#39;metric&#39;: &#39;minkowski&#39;,
 &#39;metric_params&#39;: None,
 &#39;n_jobs&#39;: None,
 &#39;n_neighbors&#39;: 5,
 &#39;p&#39;: 2,
 &#39;weights&#39;: &#39;uniform&#39;}
</pre></div>
</div>
</div>
</div>
<p>For our <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code>, there are more parameters we can adjust but the main one is the <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> which limits the number of levels (or layers of splits) in a decision tree.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>

<span class="c1"># Print all hyperparameters</span>
<span class="n">model</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.0,
 &#39;class_weight&#39;: None,
 &#39;criterion&#39;: &#39;gini&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;monotonic_cst&#39;: None,
 &#39;random_state&#39;: None,
 &#39;splitter&#39;: &#39;best&#39;}
</pre></div>
</div>
</div>
</div>
<p>If we set <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> to be small (e.g. &lt;3) then the tree will be shallow and model is only likely to capture broad, simple patterns. It may therefore be prone to <strong>underfitting</strong>.</p>
<p>In contrast, if we do not set <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, it defaults to <code class="docutils literal notranslate"><span class="pre">None</span></code> meaning that there is no limit on the height of the tree. On the one hand, this may be useful because the tree is more likely to capture more subtle patterns. However, if the tree becomes too deep, then it may begin to split the data until every sample is perfectly separated. If the dataset contains <strong>noise</strong> (which is likely), then the tree will be fitted not only to the true signal but also to <strong>noise</strong> in the data. We call this <strong>overfitting</strong> and it can reduce the overall accuracy of our model.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Machine learning models tend to be very flexible meaning that they are great at finding complex, non-linear patterns in data. The cost of such flexibility is that they will <strong>memorize</strong> or <strong>overfit</strong> the training data if not properly developed. Preventing them doing this is therefore possibly the greatest challenge in machine learning research.</p>
</div>
</section>
<section id="hyperparameter-tuning">
<h2>Hyperparameter tuning<a class="headerlink" href="#hyperparameter-tuning" title="Link to this heading">#</a></h2>
<p>So can we pick a <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> that prevents our model underfitting or overfitting our data? Yes, we can and one way to do that is to use <strong>hyperparameter tuning</strong>. We will demonstrate this timing using <strong>Decision Tree Regression</strong> where the goal is to <strong>predict a continuous variable</strong>. We will first develop a shallow tree (i.e. <code class="docutils literal notranslate"><span class="pre">max_depth=2</span></code>) to model the relationship between <code class="docutils literal notranslate"><span class="pre">Flipper</span> <span class="pre">Length</span></code> and <code class="docutils literal notranslate"><span class="pre">Body</span> <span class="pre">Mass</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Body Mass (g)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Body Mass (g)&#39;)
</pre></div>
</div>
<img alt="../../_images/200ca923b090669ee1194b098da25e73cfac40c4061683bebe9ea63bd34298fb.png" src="../../_images/200ca923b090669ee1194b098da25e73cfac40c4061683bebe9ea63bd34298fb.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>

<span class="c1"># Select features and target</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Body Mass (g)&quot;</span><span class="p">]</span>

<span class="c1"># Train-test split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize and fit the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">),</span> <span class="n">y_predicted</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Body Mass (g)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Body Mass (g)&#39;)
</pre></div>
</div>
<img alt="../../_images/d76762223237a0a4734fb0aa875ad3ce84718294aa392ef2c87e5c4fe22a82be.png" src="../../_images/d76762223237a0a4734fb0aa875ad3ce84718294aa392ef2c87e5c4fe22a82be.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remove the limit on max depth</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">),</span> <span class="n">y_predicted</span><span class="p">,</span> 
         <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Body Mass (g)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Body Mass (g)&#39;)
</pre></div>
</div>
<img alt="../../_images/f442b9470a99bc5e6c899a4cef7635152f45b1c66bc4cc325c952f8bad0e87ba.png" src="../../_images/f442b9470a99bc5e6c899a4cef7635152f45b1c66bc4cc325c952f8bad0e87ba.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">get_depth</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>10
</pre></div>
</div>
</div>
</div>
<p>Our “unconstrained” Decision Tree developed a very deep tree with 10 levels.</p>
<p>We can find the optimal <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> by iteratively searching over specified parameter values for an estimator using the <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Define parameter grid (i.e. max depth ranging from 2 to 14)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;max_depth&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">)}</span>

<span class="c1"># Define model</span>
<span class="n">tree_reg</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">()</span>

<span class="c1"># Define GridSearch estimator</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span>
    <span class="n">estimator</span><span class="o">=</span><span class="n">tree_reg</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span>
<span class="p">)</span>

<span class="c1"># Fit model</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">grid_search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;param_max_depth&#39;</span><span class="p">],</span> <span class="o">-</span><span class="n">results</span><span class="p">[</span><span class="s1">&#39;mean_test_score&#39;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Max depth&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;CV Mean Squared Error&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/139ecf6626032cd48eaac199f528f3b36a54333f41e131000264952ccb36df14.png" src="../../_images/139ecf6626032cd48eaac199f528f3b36a54333f41e131000264952ccb36df14.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit model with optimal max depth value</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;max_depth&#39;</span><span class="p">],</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">),</span> <span class="n">y_predicted</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Body Mass (g)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;Body Mass (g)&#39;)
</pre></div>
</div>
<img alt="../../_images/f8cecb5b166401319e0d7cf494ca7e17ad80ff7d3febad95b96d8402f6bb5a86.png" src="../../_images/f8cecb5b166401319e0d7cf494ca7e17ad80ff7d3febad95b96d8402f6bb5a86.png" />
</div>
</div>
<p>So, to summarize… we identified a parameter (<code class="docutils literal notranslate"><span class="pre">max_depth</span></code>) that had a large influence on the complexity of our model. We then used <strong>hyperparameter tuning</strong> to find the optimal value for this parameter. In this case, we defined <strong>optimal</strong> as the lowest <strong>mean squared error</strong>. Higher <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> caused an increase in MSE because our model <strong>overfit</strong> the data. We only had to incerease <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> to <strong>3</strong> to reduce <strong>underfitting</strong>. This kind of tuning is common practice when developing machine learning models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a complex model does not provide substantial accuracy gains compared to a simple model, then we should choose the “simplest” model (e.g. lower <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>) for our analysis.</p>
</div>
</section>
<section id="another-example-california-house-prices">
<h2>Another example - California House Prices<a class="headerlink" href="#another-example-california-house-prices" title="Link to this heading">#</a></h2>
<p>We are starting to approach the limit of our penguin dataset, so we will now use a new dataset that contains the median value of houses in an area in California. The features collected are based on some statistical averages for each region and geographic information.</p>
<p>First let’s import the data and define a decision tree regressor with no limit on <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">fetch_california_housing</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="c1"># Load data</span>
<span class="n">housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">housing</span><span class="o">.</span><span class="n">target</span>
<span class="n">target</span> <span class="o">*=</span> <span class="mi">100</span>  <span class="c1"># rescale the target in k$</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>8.3252</td>
      <td>41.0</td>
      <td>6.984127</td>
      <td>1.023810</td>
      <td>322.0</td>
      <td>2.555556</td>
      <td>37.88</td>
      <td>-122.23</td>
    </tr>
    <tr>
      <th>1</th>
      <td>8.3014</td>
      <td>21.0</td>
      <td>6.238137</td>
      <td>0.971880</td>
      <td>2401.0</td>
      <td>2.109842</td>
      <td>37.86</td>
      <td>-122.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7.2574</td>
      <td>52.0</td>
      <td>8.288136</td>
      <td>1.073446</td>
      <td>496.0</td>
      <td>2.802260</td>
      <td>37.85</td>
      <td>-122.24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5.6431</td>
      <td>52.0</td>
      <td>5.817352</td>
      <td>1.073059</td>
      <td>558.0</td>
      <td>2.547945</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.8462</td>
      <td>52.0</td>
      <td>6.281853</td>
      <td>1.081081</td>
      <td>565.0</td>
      <td>2.181467</td>
      <td>37.85</td>
      <td>-122.25</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define estimator</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># Split</span>
<span class="n">data_train</span><span class="p">,</span> <span class="n">data_test</span><span class="p">,</span> <span class="n">target_train</span><span class="p">,</span> <span class="n">target_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span> <span class="n">target_train</span><span class="p">)</span>

<span class="c1"># Predict</span>
<span class="n">target_predicted</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_test</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">target_test</span><span class="p">,</span> <span class="n">target_predicted</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The testing error of our model is </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> k$&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The testing error of our model is 46.93 k$
</pre></div>
</div>
</div>
</div>
<p>Sometimes the testing error alone does not provide sufficient information to determine whether a model is over- or under-fitting. To investigate in more detail, it is useful to track the <strong>training error</strong>. This metric is defined as the accuracy of our model when applied to the target variable (i.e house price) of the <strong>training dataset</strong> (as opposed to the testing dataset).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_train</span><span class="p">,</span> <span class="n">target_train</span><span class="p">)</span>

<span class="c1"># Predict</span>
<span class="n">target_predicted</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_train</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">target_train</span><span class="p">,</span> <span class="n">target_predicted</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The training error of our model is </span><span class="si">{</span><span class="n">score</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> k$&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tree depth:&quot;</span><span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">max_depth</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of leaves:&quot;</span><span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">n_leaves</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The training error of our model is 0.00 k$
Tree depth: 36
Number of leaves: 14842
</pre></div>
</div>
</div>
</div>
<p>We get perfect prediction with <strong>zero training error</strong>. This is because our decision tree grew so large that it was able to fully memorize our dataset. This should set off some alarm bells because it means our model is likely capturing <strong>noise</strong>.</p>
<p>We can use <strong>validation curves</strong> to pick a <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> that is more appropriate for our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">ValidationCurveDisplay</span>

<span class="n">max_depth</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">25</span><span class="p">])</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ValidationCurveDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">regressor</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">param_name</span><span class="o">=</span><span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="n">param_range</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_absolute_error&quot;</span><span class="p">,</span> <span class="n">negate_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">std_display_style</span><span class="o">=</span><span class="s2">&quot;errorbar&quot;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Maximum depth of decision tree&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Mean absolute error (k$)&quot;</span><span class="p">,)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8aaa0ecf60c2d344a0cc4715a9994dbac671fc9380ecb2271b69526792171cb8.png" src="../../_images/8aaa0ecf60c2d344a0cc4715a9994dbac671fc9380ecb2271b69526792171cb8.png" />
</div>
</div>
<ul class="simple">
<li><p>For <code class="docutils literal notranslate"><span class="pre">max_depth</span> <span class="pre">&lt;</span> <span class="pre">10</span></code>, the decision tree underfits the data. The training error and therefore the testing error are both high because the model is too constrained and cannot capture much of the variability of in house prices.</p></li>
<li><p>The decision tree generalizes the best at <code class="docutils literal notranslate"><span class="pre">max_depth</span> <span class="pre">=</span> <span class="pre">10</span></code>. It is flexible enough to capture a fraction of the variability of the target that generalizes, while not memorizing all of the noise in the target.</p></li>
<li><p>The decision tree overfits for <code class="docutils literal notranslate"><span class="pre">max_depth</span> <span class="pre">&gt;</span> <span class="pre">10</span></code>. The training error becomes very small, while the testing error actually increases. At these <code class="docutils literal notranslate"><span class="pre">max_depths</span></code>, the model is <strong>memorizing</strong> the dataset, including the noise, which is harming its ability to generalize to the test data.</p></li>
</ul>
<p>Another example of under- or over-fitting would be a polynomial function that had too few or too many degrees.</p>
<a class="reference internal image-reference" href="../../_images/underover.png"><img alt="../../_images/underover.png" class="align-center" src="../../_images/underover.png" style="width: 800px;" /></a>
</section>
<section id="noise">
<h2>Noise<a class="headerlink" href="#noise" title="Link to this heading">#</a></h2>
<p>Our house price dataset likely contains a lot of noise. The price of the house could depend time of year or the circumstances of the buyer or seller. These factors are not considered in the features. Since these missing feature are randomly varying from one sample to the next, it appears as if the target variable was changing because of the impact of a random perturbation or noise, even if there were no errors made during the data collection process.</p>
<p>The same is true for environmental data. We may not be able to measure something that impacts the target variable. It is also possible that there are instrumental errors such as poorly calibrated sensors, limitations in precision, or interference that impact the accuracy of our data. Those unpredictable data acquisition errors can happen either on the input features <strong>or</strong> in the target variable (in which case we often name this <strong>label noise</strong>).</p>
<p>While it is challenging to understand which kind of “noise” is dominating, there are several strategies for reducing the sensitivity of our models to noise:</p>
<ul class="simple">
<li><p><strong>Data augmentation</strong>: increase the number of labeled samples in our training dataset</p></li>
<li><p><strong>Regularization</strong>: enforce limits or penalize models that become too complex</p></li>
<li><p><strong>Ensemble models</strong>: combine predictions from multiple different models to average out errors from individual models</p></li>
</ul>
<p>And there are probably many other strategies…</p>
</section>
<section id="learning-curves">
<h2>Learning curves<a class="headerlink" href="#learning-curves" title="Link to this heading">#</a></h2>
<p>Besides under- and over-fitting, it is also important to understand how the accuracy of our model is influenced by the <strong>number of samples available</strong>. To investigate this source of error, we can synthetically reduce the number of samples used to train the predictive model and check the training and testing errors using a <strong>learning curve</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A learning curve could also be used to investigate how the testing error changes with number of training cycles or epochs.</p>
</div>
<p>Let’s compute the learning curve for a decision tree and vary the proportion of the training set from 10% to 100%.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_sizes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.1  , 0.325, 0.55 , 0.775, 1.   ])
</pre></div>
</div>
</div>
</div>
<p>We will use a <code class="docutils literal notranslate"><span class="pre">ShuffleSplit</span></code> cross-validation to assess our predictive model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cv</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we are all set to carry out the experiment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">LearningCurveDisplay</span>

<span class="n">display</span> <span class="o">=</span> <span class="n">LearningCurveDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">regressor</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">train_sizes</span><span class="o">=</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
    <span class="n">score_type</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_absolute_error&quot;</span><span class="p">,</span>
    <span class="n">negate_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">score_name</span><span class="o">=</span><span class="s2">&quot;Mean absolute error (k$)&quot;</span><span class="p">,</span>
    <span class="n">std_display_style</span><span class="o">=</span><span class="s2">&quot;errorbar&quot;</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9e7bdb2d9f005676194db657efe60193503bf99324f22637a60c2b9b0a009b58.png" src="../../_images/9e7bdb2d9f005676194db657efe60193503bf99324f22637a60c2b9b0a009b58.png" />
</div>
</div>
<p>We find that the testing error becomes lower as more samples are added to the training set. It looks as though the testing error is starting to plateau a bit but it is also still going down. We can therefore conclude that <strong>more samples would probably improve our predictive model</strong>.</p>
<p>This kind of analysis can be useful for informing decision-making. If it is possible to collect more samples, then we should probably do that. If that is not possible, we may consider artificially creating new, modified copies of existing data (i.e. <strong>data augmentation</strong>).</p>
<p>On the other hand, if the testing error did plateau, then collecting more samples would not be useful and we may reserve our efforts for other things e.g. reducing or handling noise in dataset, developing more sophisticated models etc.</p>
</section>
<section id="ensembles">
<h2>Ensembles<a class="headerlink" href="#ensembles" title="Link to this heading">#</a></h2>
<p><strong>Ensemble learning</strong> is a popular way to control overfitting. This is a technique where multiple individual models are combined to produce a single more robust (and often more accurate) prediction model. The two main approaches to ensemble learning are 1) <strong>bagging</strong> (i.e. Bootstrap AGGregatING) and 2) <strong>boosting</strong>. We can use these strategies to combine many Decision Trees into powerful models that generalize better.</p>
</section>
<section id="bagging">
<h2>Bagging<a class="headerlink" href="#bagging" title="Link to this heading">#</a></h2>
<p>Bagging involves fitting a base estimator (e.g. Decision Tree) to a <strong>random subset</strong> of our original dataset and then aggregating their individual predictions (either by voting or by averaging) to generate a final prediction. This approach exploits the <strong>Weak Law of Large Numbers</strong> which states that “<em>the mean of a collection of independent and identically distributed samples from a random variable converges in probability to the expected value</em>”.</p>
<p>The bagging procedure can be conceptualized using the plot below which shows our Penguin dataset. The first panel shows all the observations in our dataset. The other three panels show a random subset. To perform bagging, we would train <strong>independent models</strong> on each of these random subsets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">resample</span>

<span class="n">species_list</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Adelie&quot;</span><span class="p">:</span> <span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Chinstrap&quot;</span><span class="p">:</span> <span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="s2">&quot;Gentoo&quot;</span><span class="p">:</span> <span class="s2">&quot;orange&quot;</span><span class="p">}</span>

<span class="c1"># Create 4 subplots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Panel 1: Full data</span>
<span class="k">for</span> <span class="n">species</span> <span class="ow">in</span> <span class="n">species_list</span><span class="p">:</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">species</span><span class="p">]</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
        <span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span>
        <span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="n">species</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">species</span><span class="p">])</span>

<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Body Mass (g)&quot;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Panels 2–4: Bootstrap samples</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">):</span>
    <span class="n">sample_df</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">species</span> <span class="ow">in</span> <span class="n">species_list</span><span class="p">:</span>
        <span class="n">subset</span> <span class="o">=</span> <span class="n">sample_df</span><span class="p">[</span><span class="n">sample_df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">species</span><span class="p">]</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
            <span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Flipper Length (mm)&#39;</span><span class="p">],</span>
            <span class="n">subset</span><span class="p">[</span><span class="s1">&#39;Body Mass (g)&#39;</span><span class="p">],</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="n">species</span><span class="p">,</span>
            <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">species</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Flipper Length (mm)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/8be26a58c41c0282ecb4bbb17311b4f3a7e367fa98ff061b182f62bc2e3b0288.png" src="../../_images/8be26a58c41c0282ecb4bbb17311b4f3a7e367fa98ff061b182f62bc2e3b0288.png" />
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Scikit-learn</span></code> implements the bagging procedure as a <strong>meta-estimator</strong>: an estimator that wraps another estimator by cloning it several times and training it independently on each random subset. Below we define a meta-estimator (i.e. <code class="docutils literal notranslate"><span class="pre">BaggingClassifer</span></code>) with <strong>100 independent</strong> <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifiers</span></code> (i.e. <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaggingClassifier</span>

<span class="c1"># Define base estimator</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>

<span class="c1"># Initialize bagging &quot;meta-estimator&quot;</span>
<span class="n">bclf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Fit</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">bclf</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="n">feature_list</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Print scores</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.979
</pre></div>
</div>
</div>
</div>
<p>A note on the size of the random samples. The default parameters for the <code class="docutils literal notranslate"><span class="pre">BaggingClassifer</span></code> is <code class="docutils literal notranslate"><span class="pre">max_samples=1.0</span></code> which implies that 100% of the training dataset is used in each random subset. But <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code> indicates that our <strong>sampling involves replacement</strong>. This means that our random subset has the same number of items as our original training datset but is still different because it may <strong>duplicate</strong> some items and not select others. Hence, <strong>Bootstrap AGGregatING = Bagging</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bclf</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bootstrap&#39;: True,
 &#39;bootstrap_features&#39;: False,
 &#39;estimator__ccp_alpha&#39;: 0.0,
 &#39;estimator__class_weight&#39;: None,
 &#39;estimator__criterion&#39;: &#39;gini&#39;,
 &#39;estimator__max_depth&#39;: None,
 &#39;estimator__max_features&#39;: None,
 &#39;estimator__max_leaf_nodes&#39;: None,
 &#39;estimator__min_impurity_decrease&#39;: 0.0,
 &#39;estimator__min_samples_leaf&#39;: 1,
 &#39;estimator__min_samples_split&#39;: 2,
 &#39;estimator__min_weight_fraction_leaf&#39;: 0.0,
 &#39;estimator__monotonic_cst&#39;: None,
 &#39;estimator__random_state&#39;: None,
 &#39;estimator__splitter&#39;: &#39;best&#39;,
 &#39;estimator&#39;: DecisionTreeClassifier(),
 &#39;max_features&#39;: 1.0,
 &#39;max_samples&#39;: 1.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_jobs&#39;: None,
 &#39;oob_score&#39;: False,
 &#39;random_state&#39;: None,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Our <code class="docutils literal notranslate"><span class="pre">BaggingClassifier</span></code> picks random subsets from the data but the Decision Trees may all be quite similar because they <strong>consider all features</strong> when making splits.</p>
<p>A <code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> is a specialized version of decision tree bagging that adds another source of randomness. Instead of considering all features when making splits in the decision tree, it <strong>considers only a random subset of features</strong>. This increases the difference between decision trees, often leading to reduced overfitting and testing error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Define model</span>
<span class="n">bclf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">oob_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Fit</span>
<span class="n">bclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">feature_list</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">])</span>

<span class="c1">#Print score</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">bclf</span><span class="o">.</span><span class="n">oob_score_</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.977
</pre></div>
</div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since each tree is trained on a bootstrap sample (i.e. with replacement), about 1/3 of the data is left out during the training. These “out-of-bag” samples can be used to generate a fairly reliable estimate of the <strong>testing error</strong> without having to split the data or perform computationally expensive cross-validation.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bclf</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bootstrap&#39;: True,
 &#39;ccp_alpha&#39;: 0.0,
 &#39;class_weight&#39;: None,
 &#39;criterion&#39;: &#39;gini&#39;,
 &#39;max_depth&#39;: None,
 &#39;max_features&#39;: &#39;sqrt&#39;,
 &#39;max_leaf_nodes&#39;: None,
 &#39;max_samples&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;monotonic_cst&#39;: None,
 &#39;n_estimators&#39;: 100,
 &#39;n_jobs&#39;: None,
 &#39;oob_score&#39;: True,
 &#39;random_state&#39;: None,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Random Forests is a very popular ensemble algorithm. This is partly because it almost always performs better than single models and partly because it is not very sensitive to the choice of hyperparameters. There are really only two hyperparameters to pick, <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> which can be as large as you can computationally afford and <code class="docutils literal notranslate"><span class="pre">max_features</span></code> which determines the number of features to use for each tree. Usually the default value of <code class="docutils literal notranslate"><span class="pre">sqrt(n_features)</span></code> is appropriate. Furthermore, since decision trees work with features that have different scales or magnitudes, preprocessing is minimal.</p>
</section>
<section id="boosting">
<h2>Boosting<a class="headerlink" href="#boosting" title="Link to this heading">#</a></h2>
<p>Boosting is another ensembling approach that considers all our training data but fits models <strong>sequentially</strong> (instead of bagging independently on random subsets). To do this, it fits an initial model and then <strong>re-weights</strong> misclassified observations so that the next model will focus on classifying them correctly. Final prediction is usually a weighted average of these individual models (with more accurate models being weighted higher). Usually the individual models used in boosting are simple (e.g. shallow decision trees) and we call them <strong>weak learners</strong>.</p>
<p>In its most simplest form, boosting can be implemented by re-weighting our data such that <strong>mispredicted samples</strong> are assigned a <strong>value of 1</strong> and <strong>accurately predicted samples</strong> are assigned a <strong>value of 0</strong>. If we do this, then the new classifier will only consider misclassified samples during its training phase.</p>
<a class="reference internal image-reference" href="../../_images/boosting.png"><img alt="../../_images/boosting.png" class="align-center" src="../../_images/boosting.png" style="width: 600px;" /></a>
<p>In practice, there are statistical algorithms that have been developed for determining how sample weights should be updated. The classic version of this approach is called <code class="docutils literal notranslate"><span class="pre">AdaBoost</span></code> (i.e. Adaptive Boosting).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="c1"># Define base estimator</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Initialize boosting &quot;meta-estimator&quot;</span>
<span class="n">bclf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit</span>
<span class="n">bclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error rate of each classifier:&quot;</span><span class="p">,</span> 
      <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">err</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">err</span> <span class="ow">in</span> <span class="n">bclf</span><span class="o">.</span><span class="n">estimator_errors_</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weight of each classifier:&quot;</span><span class="p">,</span>
      <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">err</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">err</span> <span class="ow">in</span> <span class="n">bclf</span><span class="o">.</span><span class="n">estimator_weights_</span><span class="p">])</span>

<span class="c1"># Score</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">bclf</span><span class="p">,</span> <span class="n">df_scaled</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">bclf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The testing accuracy using a </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error rate of each classifier: [&#39;0.035&#39;, &#39;0.041&#39;, &#39;0.109&#39;, &#39;0.054&#39;, &#39;0.111&#39;]
Weight of each classifier: [&#39;4.01&#39;, &#39;3.83&#39;, &#39;2.80&#39;, &#39;3.56&#39;, &#39;2.77&#39;]
The testing accuracy using a AdaBoostClassifier is 0.982
</pre></div>
</div>
</div>
</div>
<p>We developed an <code class="docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code> with <strong>5</strong> different <code class="docutils literal notranslate"><span class="pre">DecisionTreeClassifiers</span></code>, each of which are shallow (i.e. <code class="docutils literal notranslate"><span class="pre">max_depth=2</span></code>). The <strong>error rate</strong> for each classifier determines the <strong>classifier’s weight</strong> in the final ensemble.</p>
</section>
<section id="gradient-boosting">
<h2>Gradient boosting<a class="headerlink" href="#gradient-boosting" title="Link to this heading">#</a></h2>
<p>While AdaBoost is a useful algorithm for learning about boosting, it is not the most computationally efficient or flexible. Nowadays, <strong>Gradient Boosting</strong> is much more popular. Gradient boosting also fits models sequentially but instead of re-weighting the mispredicted samples, it computes the <strong>residual errors</strong> between the actual and predicted samples and fits a new model to these residuals.</p>
<a class="reference internal image-reference" href="../../_images/gradient-boosting.png"><img alt="../../_images/gradient-boosting.png" class="align-center" src="../../_images/gradient-boosting.png" style="width: 500px;" /></a>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="c1"># Initialize gradient boosting classifier</span>
<span class="n">bclf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit</span>
<span class="n">bclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">])</span>

<span class="c1"># Score</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">bclf</span><span class="p">,</span> <span class="n">df_scaled</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Species&#39;</span><span class="p">],</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="n">bclf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The testing accuracy using a </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The testing accuracy using a GradientBoostingClassifier is 0.982
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bclf</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;ccp_alpha&#39;: 0.0,
 &#39;criterion&#39;: &#39;friedman_mse&#39;,
 &#39;init&#39;: None,
 &#39;learning_rate&#39;: 0.1,
 &#39;loss&#39;: &#39;log_loss&#39;,
 &#39;max_depth&#39;: 3,
 &#39;max_features&#39;: None,
 &#39;max_leaf_nodes&#39;: None,
 &#39;min_impurity_decrease&#39;: 0.0,
 &#39;min_samples_leaf&#39;: 1,
 &#39;min_samples_split&#39;: 2,
 &#39;min_weight_fraction_leaf&#39;: 0.0,
 &#39;n_estimators&#39;: 100,
 &#39;n_iter_no_change&#39;: None,
 &#39;random_state&#39;: 42,
 &#39;subsample&#39;: 1.0,
 &#39;tol&#39;: 0.0001,
 &#39;validation_fraction&#39;: 0.1,
 &#39;verbose&#39;: 0,
 &#39;warm_start&#39;: False}
</pre></div>
</div>
</div>
</div>
<p>Gradient boosting is also useful for regression. Here we apply a <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> to predict house prices. The plot below is a <strong>learning curve</strong> (or loss curve) that shows how the model’s performance improves as we increase the number of estimators in the ensemble. The performance improves rapidly to begin with since the base estimator is a <strong>weak learner</strong> and residuals are large and easy to learn. But as the residuals become smaller and smaller, newer models provide less of an improvement and the performance starts to plateau.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All Gradient Boosting algorithms use Decision Trees (also known as Classification and Regression Trees (CART)) as their base estimators.</p>
</div>
<p>This idea of fitting new weak learners to the <strong>negative gradient of the loss function</strong> explains why this class of models are called <strong>gradient boosting algorithms</strong>. By predicting the gradient, the new learners compute the direction <strong>and</strong> magnitude of adjustment needed to reduce the loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="c1"># Initialize gradient boosting regressor</span>
<span class="n">breg</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">()</span>

<span class="n">n_estimators</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">ValidationCurveDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
    <span class="n">breg</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">param_name</span><span class="o">=</span><span class="s2">&quot;n_estimators&quot;</span><span class="p">,</span> <span class="n">param_range</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="s2">&quot;neg_mean_absolute_error&quot;</span><span class="p">,</span> <span class="n">negate_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">std_display_style</span><span class="o">=</span><span class="s2">&quot;errorbar&quot;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">disp</span><span class="o">.</span><span class="n">ax_</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Number of estimators&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Mean absolute error (k$)&quot;</span><span class="p">,)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/96c660a838afea5653a6006f290de59724440c53b5a462f8fd2670f8c1c6dc4f.png" src="../../_images/96c660a838afea5653a6006f290de59724440c53b5a462f8fd2670f8c1c6dc4f.png" />
</div>
</div>
<p>Gradient Boosting algorithms (e.g. <strong>XGBoost</strong>, <strong>CatBoost</strong>) have emerged as some of the most powerful and widely used methods for machine learning, often outperforming deep learning models. They consistently rank at the top of benchmark datasets and machine learning competitions, especially for tasks involving <strong>structured</strong> (i.e. tabular) data.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p><strong>Bagging</strong></p></th>
<th class="head"><p><strong>Boosting</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Full name</strong></p></td>
<td><p>Bootstrap Aggregating</p></td>
<td><p>Adaptive Boosting</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Main goal</strong></p></td>
<td><p>Reduce <strong>variance</strong> (i.e. overfitting)</p></td>
<td><p>Reduce <strong>bias</strong> (i.e. underfitting)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Learning process</strong></p></td>
<td><p>Models are trained <strong>independently and in parallel</strong></p></td>
<td><p>Models are trained <strong>sequentially</strong>, each correcting the previous one</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Data sampling</strong></p></td>
<td><p>Uses <strong>bootstrap samples</strong> (random sampling with replacement)</p></td>
<td><p>Uses <strong>all data</strong>, but increases weight on misclassified data</p></td>
</tr>
<tr class="row-even"><td><p><strong>Base learners</strong></p></td>
<td><p>Usually high-variance models (e.g. deep decision trees)</p></td>
<td><p>Usually weak learners (e.g. shallow decision trees)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Model diversity</strong></p></td>
<td><p>Comes from random sampling of data/features</p></td>
<td><p>Comes from focusing on “hard” cases (errors or gradients)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Parallelization</strong></p></td>
<td><p>Easy to parallelize (since learners are independent)</p></td>
<td><p>Harder to parallelize (since each step depends on previous)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Algorithms</strong></p></td>
<td><p>Random Forests</p></td>
<td><p>AdaBoost, Gradient Boosting, XGBoost</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="further-reading">
<h2>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>Scikit-learn developers. (2022) <a class="reference external" href="https://inria.github.io/scikit-learn-mooc/index.html">Machine learning in Python with scikit-learn MOOC</a></p>
<p>Weinberger, K. (2018). <a class="reference external" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/">Machine Learning for Intelligent Systems</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lectures/unit2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../../labs/assignment4.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Assignment 4</p>
      </div>
    </a>
    <a class="right-next"
       href="../../activities/wines.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Wine activity</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">What is machine learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-the-palmer-penguins">Example - the Palmer Penguins</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-vocabularly">Some vocabularly</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning">Types of machine learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">Supervised learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning">Unsupervised learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning">Reinforcement learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-machine-learning-tasks">Common machine learning tasks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification">Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression">Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">Clustering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploring-our-dataset">Exploring our dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">Feature scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-testing-subsets">Training and testing subsets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-our-first-model">Fitting our first model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">Cross-validation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-tree-classifier">Decision Tree Classifier</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-or-error-functions">Loss (or error) functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning">Hyperparameter tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#another-example-california-house-prices">Another example - California House Prices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#noise">Noise</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-curves">Learning curves</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensembles">Ensembles</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bagging">Bagging</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#boosting">Boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-boosting">Gradient boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further reading</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Johnny Ryan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>